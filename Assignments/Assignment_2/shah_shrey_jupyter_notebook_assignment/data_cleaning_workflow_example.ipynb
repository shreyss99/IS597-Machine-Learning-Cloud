{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Cleaning Workflow Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Requirements\n",
    "This notebook requires the Anaconda virtual env that was created\n",
    "during the first week of this course and named e4_trainor_python_course.\n",
    "\n",
    "No notebooks are expected to be run before this one.\n",
    "While at least one data analysis notebook is expected to be run after this notebook, it has not yet been developed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Overview\n",
    "This sample workflow cleans a raw data .CSV file to produce a\n",
    "cleaned data file.\n",
    "\n",
    "The .CSV file contains a header row.\n",
    "\n",
    "Each data row contains three fields separated by commas (City,\n",
    "State, Quantity). We suspect coding errors in the City and State\n",
    "fields. We will not be cleaning the Quantity field.\n",
    "\n",
    "This workflow has the following parts:\n",
    "• Count City Values (with raw data)\n",
    "• Count State Values (with raw data)\n",
    "• Correct Data Coding Errors\n",
    "• Count City Values (with cleaned data)\n",
    "• Count State Values (with cleaned data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Raw Data\n",
    "### Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-11T06:10:12.659914Z",
     "start_time": "2024-02-11T06:10:12.650785Z"
    }
   },
   "outputs": [],
   "source": [
    "# set variables to be passed to the Count City Values function.\n",
    "data_directory = 'data'\n",
    "input_filename = 'raw_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Count City Values (with raw data)\n",
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-11T06:10:12.692481Z",
     "start_time": "2024-02-11T06:10:12.668406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARLINGTON: 3\n",
      "Arlington: 50\n",
      "BRISTOL: 3\n",
      "Bristol: 41\n",
      "Bristol : 3\n",
      "CENTERVILLE: 3\n",
      "CHESTER: 1\n",
      "CLINTON: 1\n",
      "Centerville: 46\n",
      "Centerville : 3\n",
      "Chester: 40\n",
      "Chester : 2\n",
      "Clinton: 36\n",
      "Clinton : 4\n",
      "DOVER: 1\n",
      "Dayton: 37\n",
      "Dayton : 3\n",
      "Dover: 52\n",
      "FRANKLIN: 1\n",
      "Fairview: 50\n",
      "Fairview : 2\n",
      "Franklin: 58\n",
      "GEORGETOWN: 5\n",
      "GREENVILLE: 5\n",
      "Georgetown: 42\n",
      "Georgetown : 1\n",
      "Greenville: 41\n",
      "LEBANON: 3\n",
      "Lebanon: 43\n",
      "Lebanon : 1\n",
      "MADISON: 1\n",
      "MILTON: 3\n",
      "Madison: 52\n",
      "Madison : 3\n",
      "Milton: 41\n",
      "Milton : 1\n",
      "NEWPORT: 2\n",
      "Newport: 42\n",
      "Newport : 2\n",
      "Oakland: 51\n",
      "Oakland : 1\n",
      "SALEM: 1\n",
      "SPRINGFIELD: 1\n",
      "Salem: 42\n",
      "Salem : 1\n",
      "Springfield: 52\n",
      "WASHINGTON: 8\n",
      "Washington: 36\n",
      "Washington : 3\n",
      "Winchester: 46\n",
      "Winchester : 2\n",
      "arlington: 2\n",
      "bristol: 1\n",
      "chester: 2\n",
      "clinton: 1\n",
      "dayton: 1\n",
      "fairview: 5\n",
      "lebanon: 1\n",
      "madison: 2\n",
      "newport: 3\n",
      "salem: 2\n",
      "washington: 4\n",
      "winchester: 4\n"
     ]
    }
   ],
   "source": [
    "# import and call the Count City Values function using variables that point to the raw data.\n",
    "from count_city_name_values import do_count_city_name_values\n",
    "do_count_city_name_values(data_directory, input_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Count State Values (with raw data)\n",
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-11T06:10:12.692998Z",
     "start_time": "2024-02-11T06:10:12.676751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALIFORNIA: 5\n",
      "California: 84\n",
      "California : 1\n",
      "Florida: 115\n",
      "GEORGIA: 5\n",
      "Georgia: 88\n",
      "Georgia : 3\n",
      "ILLINOIS: 5\n",
      "Illinois: 78\n",
      "Illinois : 7\n",
      "MICHIGAN: 6\n",
      "Michigan: 84\n",
      "Michigan : 5\n",
      "NEW YORK: 4\n",
      "NORTH CAROLINA: 6\n",
      "New York: 106\n",
      "New York : 5\n",
      "North Carolina: 90\n",
      "North Carolina : 1\n",
      "OHIO: 3\n",
      "Ohio: 84\n",
      "PENNSYLVANIA: 3\n",
      "Pennsylvania: 77\n",
      "Pennsylvania : 5\n",
      "TEXAS: 3\n",
      "Texas: 85\n",
      "Texas : 3\n",
      "california: 5\n",
      "florida: 4\n",
      "georgia: 1\n",
      "illinois: 4\n",
      "michigan: 3\n",
      "new york: 4\n",
      "north carolina: 7\n",
      "ohio: 7\n",
      "texas: 4\n"
     ]
    }
   ],
   "source": [
    "# import and call the Count State Values function using variables that point to the raw data.\n",
    "from count_state_name_values import do_count_state_name_values\n",
    "do_count_state_name_values(data_directory, input_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Correct Data Coding Errors\n",
    "### Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-11T06:10:12.693281Z",
     "start_time": "2024-02-11T06:10:12.683288Z"
    }
   },
   "outputs": [],
   "source": [
    "# set additional variable to be passed to the Clean Data Coding Errors function.\n",
    "output_filename = 'cleaned_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-11T06:10:12.742737Z",
     "start_time": "2024-02-11T06:10:12.689949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 cleaned records were written to data/cleaned_data.csv.\n"
     ]
    }
   ],
   "source": [
    "# Import and call the Clean Data Coding Error function.\n",
    "from clean_data_coding_errors import do_clean_data_coding_errors\n",
    "do_clean_data_coding_errors(data_directory, input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Count City Values (with cleaned data)\n",
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-11T06:10:12.743947Z",
     "start_time": "2024-02-11T06:10:12.702581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arlington: 55\n",
      "Bristol: 48\n",
      "Centerville: 52\n",
      "Chester: 45\n",
      "Clinton: 42\n",
      "Dayton: 41\n",
      "Dover: 53\n",
      "Fairview: 57\n",
      "Franklin: 59\n",
      "Georgetown: 48\n",
      "Greenville: 46\n",
      "Lebanon: 48\n",
      "Madison: 58\n",
      "Milton: 45\n",
      "Newport: 49\n",
      "Oakland: 52\n",
      "Salem: 46\n",
      "Springfield: 53\n",
      "Washington: 51\n",
      "Winchester: 52\n"
     ]
    }
   ],
   "source": [
    "# Call the Count City Values function using variables that point to the cleaned data.\n",
    "do_count_city_name_values(data_directory, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Count State Values (with cleaned data)\n",
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-11T06:10:12.744389Z",
     "start_time": "2024-02-11T06:10:12.709293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California: 95\n",
      "Florida: 119\n",
      "Georgia: 97\n",
      "Illinois: 94\n",
      "Michigan: 98\n",
      "New York: 119\n",
      "North Carolina: 104\n",
      "Ohio: 94\n",
      "Pennsylvania: 85\n",
      "Texas: 95\n"
     ]
    }
   ],
   "source": [
    "# Call the Count State Values function using variables that point to the cleaned data.\n",
    "do_count_state_name_values(data_directory, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Possible Enhancement: Clean Quantity Field"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "• How might we determine a proper upper limit value for Quantity?\n",
    "\n",
    "Ans) We can find the overall maximum value for the entire Quantity column and use it as a reference upper limit. It may be possible that, due to data entry issues, some values may be extremely out of range. Such rows can be excluded or replaced with a proportionate value.\n",
    "&nbsp;\n",
    "\n",
    "• How might we determine a proper lower limit value for Quantity?\n",
    "\n",
    "Ans) Since Quantity is a kind of count, we first need to find all the rows that have a negative value for the Quantity field. For such rows, we can replace them with 0 or decide to remove them. Once this is done, we can find the overall minimum value for the entire Quantity column and use it as a reference lower limit.\n",
    "&nbsp;\n",
    "\n",
    "• What value might we use (if any) to replace entries over the upper limit?\n",
    "\n",
    "Ans) For entries that are higher than the upper limit, they can be replaced with the higher limit that we decide for the Quantity field (which can be the maximum value). If we cannot replace the value with something else, we can understand the use case and decide if we can remove such rows.\n",
    "&nbsp;\n",
    "\n",
    "• What value might we use (if any) to replace entries under the lower limit?\n",
    "\n",
    "Ans) For entries that are lower than the lower limit, they can be replaced with the lower limit that we decide for the Quantity field (which can be 0). If we cannot replace the value with something else, we can understand the use case and decide if we can remove such rows.\n",
    "&nbsp;\n",
    "\n",
    "• How would we treat entries that were not numeric (like “Hi, Mom!)?\n",
    "\n",
    "Ans) Non-numeric entries can be treated by either filtering them out or converting them into a suitable format for analysis. This can be done by performing text analytics, converting the data into a vector format for further use, or encoding them as categorical variables. The approach depends on the use case of the analysis.\n",
    "&nbsp;\n",
    "\n",
    "• Should we consider just dropping records that don’t fall within our standards?\n",
    "\n",
    "Ans) Dropping records that do not meet the defined standards is not always recommended. If a large proportion of the data does not meet the standards, eliminating it can lead to substantial data loss. On the other hand, it can give insight into a flawed data collection process. If the data available to us has a small sample size, it does not make sense to exclude that data.\n",
    "But if data quality and compliance with standards are not a big issue, then we can think of eliminating such data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-11T06:10:12.744776Z",
     "start_time": "2024-02-11T06:10:12.716920Z"
    }
   },
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
