{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b420ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23326e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, colname):\n",
    "    \"\"\"\n",
    "    Read in input file and load data\n",
    "\n",
    "    filename: csv file\n",
    "    colname: column name for texts\n",
    "    return: X and y dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    ## 1. Read in data from input file\n",
    "    df = pd.read_csv(filename, sep=\"\\t\", encoding='utf-8')\n",
    "    \n",
    "    print(\"************** Loading Data ************\", \"\\n\")\n",
    "\n",
    "    # Check number of rows and columns\n",
    "    print(\"No of Rows: {}\".format(df.shape[0]))\n",
    "    print(\"No of Columns: {}\".format(df.shape[1]))\n",
    "\n",
    "    ## 2. Select data needed for processing\n",
    "    print(f\"Selecting columns needed for processing: pmid, {colname}, rct\", \"\\n\")\n",
    "    df = df[['pmid', colname, 'rct']]\n",
    "    \n",
    "\n",
    "    ## 3. Cleaning data\n",
    "    # Trim unnecessary spaces for strings\n",
    "    df[colname] = df[colname].apply(lambda x: str(x))\n",
    "\n",
    "    # 3-1. Remove null values\n",
    "    df=df.dropna()\n",
    "\n",
    "    # Check number of rows and columns\n",
    "    print(\"No of rows (After dropping null): {}\".format(df.shape[0]))\n",
    "    print(\"No of columns: {}\".format(df.shape[1]))\n",
    "\n",
    "    # 3-2. Remove duplicates and keep first occurrence\n",
    "    df.drop_duplicates(subset=['pmid'], keep='first', inplace=True)\n",
    "\n",
    "    # Check number of rows and columns\n",
    "    print(\"No of rows (After removing duplicates): {}\".format(df.shape[0]))\n",
    "\n",
    "    # Check the first few instances\n",
    "    print(\"\\n<Data View: First Few Instances>\\n\")\n",
    "    print(df.head(5))\n",
    "    \n",
    "    # 3-3. Check label class\n",
    "    print('\\nClass Counts(label, row): Total')\n",
    "    print(df[\"rct\"].value_counts())\n",
    "    \n",
    "\n",
    "    ## 4. Split into X and y (target)\n",
    "    X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf64ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X_data, y_data):\n",
    "    \"\"\"\n",
    "    Read in the X_data and y_data and split into train, validtion, and test sets.\n",
    "\n",
    "    X_data: dataframe consisting of only the input features\n",
    "    y_data: series consisting of only the output label\n",
    "    return: a tuple of the split data consisting of train, test and validation sets for both X_data and y_data\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n************** Spliting Data **************\\n\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42, stratify=y_data)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test,y_test, test_size=0.5, random_state=42, stratify=y_test)\n",
    "\n",
    "    ## Check the data view of each data set\n",
    "\n",
    "    ## Data Shape\n",
    "    print(\"Train Data: {}\".format(X_train.shape))\n",
    "    print(\"Val Data: {}\".format(X_val.shape))\n",
    "    print(\"Test Data: {}\".format(X_test.shape))\n",
    "\n",
    "    ## Label Distribution\n",
    "    print('\\nClass Counts(label, row): Train')\n",
    "    print(y_train.value_counts())\n",
    "    print('\\nClass Counts(label, row): Validation')\n",
    "    print(y_val.value_counts())\n",
    "    print('\\nClass Counts(label, row): Test')\n",
    "    print(y_test.value_counts())\n",
    "\n",
    "    ## Display the first 3 instances of X data\n",
    "    print(\"\\nData View: X Train\")\n",
    "    print(X_train.head(3))\n",
    "    print(\"\\nData View: X Val\")\n",
    "    print(X_val.head(3))\n",
    "    print(\"\\nData View: X Test\")\n",
    "    print(X_test.head(3))\n",
    "\n",
    "    ## Reset index\n",
    "\n",
    "    print(\"\\n************** Resetting Index **************\\n\")\n",
    "\n",
    "    # Train Data\n",
    "    X_train=X_train.reset_index(drop=True)\n",
    "    y_train=y_train.reset_index(drop=True)\n",
    "\n",
    "    # Validation Data\n",
    "    X_val=X_val.reset_index(drop=True)\n",
    "    y_val=y_val.reset_index(drop=True)\n",
    "\n",
    "    # Test Data\n",
    "    X_test=X_test.reset_index(drop=True)\n",
    "    y_test=y_test.reset_index(drop=True)\n",
    "\n",
    "    ## Check data\n",
    "\n",
    "    ## Data Shape\n",
    "    print(\"Train Data: {}\".format(X_train.shape))\n",
    "    print(\"Validation Data: {}\".format(X_val.shape))\n",
    "    print(\"Test Data: {}\".format(X_test.shape))\n",
    "\n",
    "    ## Label Distribution\n",
    "    print('\\nClass Counts(label, row): Train\\n')\n",
    "    print(y_train.value_counts())\n",
    "    print('\\nClass Counts(label, row): Val\\n')\n",
    "    print(y_val.value_counts())\n",
    "    print('\\nClass Counts(label, row): Test\\n')\n",
    "    print(y_test.value_counts())\n",
    "\n",
    "    ## Display the first 3 instances of X data\n",
    "    print(\"\\nData View: X Train\")\n",
    "    print(X_train.head(3))\n",
    "    print(\"\\nData View: X Val\")\n",
    "    print(X_val.head(3))\n",
    "    print(\"\\nData View: X Test\")\n",
    "    print(X_test.head(3))\n",
    "    \n",
    "    return (X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "026cdf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X_data_raw):\n",
    "    \"\"\"\n",
    "       Function to preprocess data with lowercase conversion, punctuation removal, tokenization, stemming\n",
    "\n",
    "       X_data_raw: X data in dataframe\n",
    "       return: transformed dataframe\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n************** Pre-processed Data **************\\n\")\n",
    "    \n",
    "    X_data=X_data_raw.iloc[:, -1].astype(str)\n",
    "    print(f\"\\nTrain Data: {X_data.shape}\")\n",
    "    \n",
    "    ## 1. convert all characters to lowercase\n",
    "    X_data = X_data.map(lambda x: x.lower())\n",
    "\n",
    "    ## 2. remove punctuation\n",
    "    X_data = X_data.str.replace('[^\\w\\s]', '')\n",
    "\n",
    "    ## 3. tokenize sentence\n",
    "    X_data = X_data.apply(nltk.word_tokenize)\n",
    "\n",
    "    ## 4. remove stopwords\n",
    "    stopword_list = stopwords.words(\"english\")\n",
    "    X_data = X_data.apply(lambda x: [word for word in x if word not in stopword_list])\n",
    "\n",
    "    ## 5. stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    X_data = X_data.apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "    ## 6. removing unnecessary space\n",
    "    X_data = X_data.apply(lambda x: \" \".join(x))\n",
    "\n",
    "    # Check data view\n",
    "    print(\"\\nData View: X Train\\n\")\n",
    "    print(X_data.head(3))\n",
    "\n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72c9f92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
