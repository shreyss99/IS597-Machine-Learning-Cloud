{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea178959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78a29e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a629235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "278b66b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_new(filename, colname, log_file):\n",
    "    \"\"\"\n",
    "    Read in input file and load data\n",
    "\n",
    "    filename: csv file\n",
    "    colname: column name for texts\n",
    "    log_file: file object to write log messages\n",
    "    return: X and y dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    ## 1. Read in data from input file\n",
    "    df = pd.read_csv(filename, sep=\"\\t\", encoding='utf-8')\n",
    "    \n",
    "    log_file.write(\"************** Loading Data ************\\n\\n\")\n",
    "\n",
    "    # Check number of rows and columns\n",
    "    log_file.write(\"No of Rows: {}\\n\".format(df.shape[0]))\n",
    "    log_file.write(\"No of Columns: {}\\n\\n\".format(df.shape[1]))\n",
    "\n",
    "    ## 2. Select data needed for processing\n",
    "    log_file.write(f\"Selecting columns needed for processing: pmid, {colname}, rct\\n\\n\")\n",
    "    df = df[['pmid', colname, 'rct']]\n",
    "    \n",
    "\n",
    "    ## 3. Cleaning data\n",
    "    # Trim unnecessary spaces for strings\n",
    "    df[colname] = df[colname].apply(lambda x: str(x))\n",
    "\n",
    "    # 3-1. Remove null values\n",
    "    df=df.dropna()\n",
    "\n",
    "    # Check number of rows and columns\n",
    "    log_file.write(\"No of rows (After dropping null): {}\\n\".format(df.shape[0]))\n",
    "    log_file.write(\"No of columns: {}\\n\\n\".format(df.shape[1]))\n",
    "\n",
    "    # 3-2. Remove duplicates and keep first occurrence\n",
    "    df.drop_duplicates(subset=['pmid'], keep='first', inplace=True)\n",
    "\n",
    "    # Check number of rows and columns\n",
    "    log_file.write(\"No of rows (After removing duplicates): {}\\n\\n\".format(df.shape[0]))\n",
    "\n",
    "    # Check the first few instances\n",
    "    log_file.write(\"<Data View: First Few Instances>\\n\\n\")\n",
    "    log_file.write(df.head(5).to_string(index=False) + \"\\n\\n\")\n",
    "    \n",
    "    # 3-3. Check label class\n",
    "    log_file.write('Class Counts(label, row): Total\\n')\n",
    "    log_file.write(df[\"rct\"].value_counts().to_string() + \"\\n\\n\")\n",
    "    \n",
    "\n",
    "    ## 4. Split into X and y (target)\n",
    "    X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be875b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_new(X_data, y_data, log_file):\n",
    "    \"\"\"\n",
    "    Read in the X_data and y_data and split into train, validation, and test sets.\n",
    "\n",
    "    X_data: dataframe consisting of only the input features\n",
    "    y_data: series consisting of only the output label\n",
    "    log_file: file object to write log messages\n",
    "    return: a tuple of the split data consisting of train, test and validation sets for both X_data and y_data\n",
    "    \"\"\"\n",
    "\n",
    "    log_file.write(\"\\n************** Splitting Data **************\\n\\n\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42, stratify=y_data)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42, stratify=y_test)\n",
    "\n",
    "    # Data Shape\n",
    "    log_file.write(\"Train Data: {}\\n\".format(X_train.shape))\n",
    "    log_file.write(\"Val Data: {}\\n\".format(X_val.shape))\n",
    "    log_file.write(\"Test Data: {}\\n\\n\".format(X_test.shape))\n",
    "\n",
    "    # Label Distribution\n",
    "    log_file.write('Class Counts(label, row): Train\\n')\n",
    "    log_file.write(y_train.value_counts().to_string() + \"\\n\\n\")\n",
    "    log_file.write('Class Counts(label, row): Validation\\n')\n",
    "    log_file.write(y_val.value_counts().to_string() + \"\\n\\n\")\n",
    "    log_file.write('Class Counts(label, row): Test\\n')\n",
    "    log_file.write(y_test.value_counts().to_string() + \"\\n\\n\")\n",
    "\n",
    "    # Display the first 3 instances of X data\n",
    "    log_file.write(\"Data View: X Train\\n\")\n",
    "    log_file.write(X_train.head(3).to_string(index=False) + \"\\n\\n\")\n",
    "    log_file.write(\"Data View: X Val\\n\")\n",
    "    log_file.write(X_val.head(3).to_string(index=False) + \"\\n\\n\")\n",
    "    log_file.write(\"Data View: X Test\\n\")\n",
    "    log_file.write(X_test.head(3).to_string(index=False) + \"\\n\\n\")\n",
    "\n",
    "    log_file.write(\"************** Resetting Index **************\\n\\n\")\n",
    "\n",
    "    # Reset index\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    X_val = X_val.reset_index(drop=True)\n",
    "    y_val = y_val.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    # Data Shape after resetting index\n",
    "    log_file.write(\"Train Data: {}\\n\".format(X_train.shape))\n",
    "    log_file.write(\"Validation Data: {}\\n\".format(X_val.shape))\n",
    "    log_file.write(\"Test Data: {}\\n\\n\".format(X_test.shape))\n",
    "\n",
    "    # Label Distribution after resetting index\n",
    "    log_file.write('Class Counts(label, row): Train\\n')\n",
    "    log_file.write(y_train.value_counts().to_string() + \"\\n\\n\")\n",
    "    log_file.write('Class Counts(label, row): Validation\\n')\n",
    "    log_file.write(y_val.value_counts().to_string() + \"\\n\\n\")\n",
    "    log_file.write('Class Counts(label, row): Test\\n')\n",
    "    log_file.write(y_test.value_counts().to_string() + \"\\n\\n\")\n",
    "\n",
    "    # Display the first 3 instances of X data after resetting index\n",
    "    log_file.write(\"Data View: X Train\\n\")\n",
    "    log_file.write(X_train.head(3).to_string(index=False) + \"\\n\\n\")\n",
    "    log_file.write(\"Data View: X Val\\n\")\n",
    "    log_file.write(X_val.head(3).to_string(index=False) + \"\\n\\n\")\n",
    "    log_file.write(\"Data View: X Test\\n\")\n",
    "    log_file.write(X_test.head(3).to_string(index=False) + \"\\n\\n\")\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "787baa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_new(X_data_raw, log_file):\n",
    "    \"\"\"\n",
    "    Function to preprocess data with lowercase conversion, punctuation removal, tokenization, stemming\n",
    "\n",
    "    X_data_raw: X data in dataframe\n",
    "    log_file: file object to write log messages\n",
    "    return: transformed dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    log_file.write(\"\\n************** Pre-processed Data **************\\n\")\n",
    "\n",
    "    X_data = X_data_raw.iloc[:, -1].astype(str)\n",
    "    log_file.write(f\"\\nData Shape: {X_data.shape}\\n\")\n",
    "\n",
    "    # 1. convert all characters to lowercase\n",
    "    X_data = X_data.map(lambda x: x.lower())\n",
    "\n",
    "    # 2. remove punctuation\n",
    "    X_data = X_data.str.replace('[^\\w\\s]', '')\n",
    "\n",
    "    # 3. tokenize sentence\n",
    "    X_data = X_data.apply(nltk.word_tokenize)\n",
    "\n",
    "    # 4. remove stopwords\n",
    "    stopword_list = stopwords.words(\"english\")\n",
    "    X_data = X_data.apply(lambda x: [word for word in x if word not in stopword_list])\n",
    "\n",
    "    # 5. stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    X_data = X_data.apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "    # 6. removing unnecessary space\n",
    "    X_data = X_data.apply(lambda x: \" \".join(x))\n",
    "\n",
    "    # Check data view\n",
    "    log_file.write(\"\\nData View:\\n\")\n",
    "    log_file.write(X_data.head(3).to_string(index=False) + \"\\n\")\n",
    "\n",
    "    return X_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddf7fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_new(X, y, modelname, log_file):\n",
    "    \"\"\"\n",
    "    Fits a machine learning model to input data.\n",
    "\n",
    "    Parameters:\n",
    "    X: Input features.\n",
    "    y: Target variable.\n",
    "    modelname: Name of the machine learning algorithm to use.\n",
    "               Choose from: Decision Tree, Logistic Regression, Support Vector Machines, Random Forest.\n",
    "    log_file: file object to write log messages\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mapping modelname to corresponding machine learning algorithm\n",
    "    models = {\n",
    "        'Decision_tree': DecisionTreeClassifier(),\n",
    "        'Logistic_regression': LogisticRegression(),\n",
    "        'Support_vector_machine': SVC(),\n",
    "        'Random_forest': RandomForestClassifier()\n",
    "    }\n",
    "    \n",
    "    # Checking if the specified modelname is valid\n",
    "    if modelname not in models:\n",
    "        raise ValueError(\"Invalid modelname. Choose from: Decision_tree, Logistic_regression, Support_vector_machine, Random_forest.\")\n",
    "    \n",
    "    # Fitting the selected model to the data\n",
    "    \n",
    "    log_file.write(f\"\\n************** Training Model: {modelname} **************\\n\")\n",
    "    \n",
    "    model = models[modelname]\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c743a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_new(y_pred, y_true, log_file):\n",
    "    \"\"\"\n",
    "    Computes the confusion matrix for evaluating model performance.\n",
    "\n",
    "    Parameters:\n",
    "    y_pred Predicted labels.\n",
    "    y_true: Actual labels.\n",
    "    log_file: file object to write log messages\n",
    "\n",
    "    Returns:\n",
    "    array: Confusion matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    log_file.write(\"\\n************** Model Evaluation **************\\n\")\n",
    "    log_file.write(\"\\nConfusion Matrix:\\n\")\n",
    "    \n",
    "    # Computing the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Writing the confusion matrix to the log file\n",
    "    log_file.write(str(cm) + '\\n')\n",
    "    \n",
    "    return cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84868ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
